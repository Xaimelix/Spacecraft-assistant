# Настройка оффлайн режима

## Что было сделано

Приложение теперь поддерживает полностью оффлайн работу через локальные LLM модели. Система автоматически выбирает доступную модель в следующем порядке приоритета:

1. **Ollama** (оффлайн) - рекомендуется для простоты
2. **HuggingFace локальные модели** (оффлайн) - для продвинутых сценариев
3. **OpenAI/Timeweb API** (онлайн) - fallback, если оффлайн модели недоступны

## Быстрый старт с Ollama

### Шаг 1: Установите Ollama

Скачайте и установите Ollama с официального сайта: https://ollama.ai/

### Шаг 2: Скачайте модель

Откройте терминал и выполните:

```bash
ollama pull llama3.2
```

Или другую модель по вашему выбору (например, `mistral`, `llama3`, `phi3`).

### Шаг 3: Убедитесь, что Ollama запущен

Ollama должен работать в фоновом режиме. Проверьте:

```bash
ollama list
```

Если Ollama не запущен, он запустится автоматически при первом запросе.

### Шаг 4: Установите зависимости

```bash
pip install -r requirements.txt
```

### Шаг 5: Запустите приложение

```bash
python app.py
```

Приложение автоматически обнаружит Ollama и будет использовать его для генерации ответов!

## Настройка через переменные окружения

В файле `config.env` вы можете настроить:

```env
# Ollama настройки (опционально)
OLLAMA_MODEL=llama3.2
OLLAMA_BASE_URL=http://localhost:11434

# Или используйте локальную HuggingFace модель
HF_MODEL=microsoft/DialoGPT-medium

# Или онлайн API (fallback)
timeweb_api=your_api_key
timeweb_openai_url=https://api.example.com
OPENAI_MODEL=gpt-5-nano
```

## Как это работает

1. При первом запросе в чате система проверяет доступность моделей
2. Если Ollama доступен - используется он (оффлайн)
3. Если Ollama недоступен, но указана HF_MODEL - загружается локальная модель HuggingFace
4. Если ничего не доступно - используется онлайн API (если настроен)
5. Если ничего не настроено - возвращается простой ответ на основе найденного контекста

## Преимущества оффлайн режима

- ✅ Полная конфиденциальность - данные не отправляются в облако
- ✅ Работа без интернета
- ✅ Нет ограничений по количеству запросов
- ✅ Быстрые ответы (локальная обработка, зависит от мощьностей оборудования)
- ✅ Бесплатно (не требуются API ключи)

## Рекомендуемые модели Ollama

- `llama3.2` - хороший баланс качества и скорости (2B параметров)
- `llama3.2:3b` - более быстрая версия
- `gemma3:1b` - качественная, быстрая модель от Google
- `mistral` - качественная модель
- `phi3` - компактная и быстрая модель

## Устранение проблем

### Ollama не обнаруживается

1. Убедитесь, что Ollama запущен: `ollama list`
2. Проверьте, что модель скачана: `ollama list`
3. Проверьте URL в `OLLAMA_BASE_URL` (по умолчанию `http://localhost:11434`)

### Медленные ответы

- Используйте более легкие модели (например, `llama3.2:1b`)
- Убедитесь, что у вас достаточно RAM (рекомендуется 8GB+)
- Для GPU ускорения установите CUDA версию PyTorch

### Ошибки при загрузке HuggingFace моделей

- Убедитесь, что у вас достаточно места на диске (модели могут быть большими)
- Проверьте подключение к интернету (для первой загрузки)
- Используйте более легкие модели для CPU

